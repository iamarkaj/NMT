{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense,GRU,Bidirectional,Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>TRAINING</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH='data.csv'\n",
    "CHECKPOINT_PATH='training_checkpoints'\n",
    "\n",
    "MAX_WORDS=14\n",
    "SAMPLES=50000\n",
    "VOCAB=15000\n",
    "BATCH_SIZE=128\n",
    "EPOCH=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'd like to tell you about one such child,</td>\n",
       "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This percentage is even greater than the perce...</td>\n",
       "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what we really mean is that they're bad at not...</td>\n",
       "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.The ending portion of these Vedas is called U...</td>\n",
       "      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    english_sentence  \\\n",
       "0  politicians do not have permission to do what ...   \n",
       "1         I'd like to tell you about one such child,   \n",
       "2  This percentage is even greater than the perce...   \n",
       "3  what we really mean is that they're bad at not...   \n",
       "4  .The ending portion of these Vedas is called U...   \n",
       "\n",
       "                                      hindi_sentence  \n",
       "0  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n",
       "1  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n",
       "2   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n",
       "3     हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n",
       "4        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=pd.read_csv(DATA_PATH)\n",
    "train=train.drop(\"source\",axis=1)\n",
    "train=train.dropna() #drop NA rows\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Samples:  127605\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Samples: \",len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count length of english and hindi sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "      <th>english_sentence_length</th>\n",
       "      <th>hindi_sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'd like to tell you about one such child,</td>\n",
       "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This percentage is even greater than the perce...</td>\n",
       "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what we really mean is that they're bad at not...</td>\n",
       "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.The ending portion of these Vedas is called U...</td>\n",
       "      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    english_sentence  \\\n",
       "0  politicians do not have permission to do what ...   \n",
       "1         I'd like to tell you about one such child,   \n",
       "2  This percentage is even greater than the perce...   \n",
       "3  what we really mean is that they're bad at not...   \n",
       "4  .The ending portion of these Vedas is called U...   \n",
       "\n",
       "                                      hindi_sentence  english_sentence_length  \\\n",
       "0  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...                       12   \n",
       "1  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...                        9   \n",
       "2   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।                       10   \n",
       "3     हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते                       12   \n",
       "4        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।                        9   \n",
       "\n",
       "   hindi_sentence_length  \n",
       "0                     14  \n",
       "1                     11  \n",
       "2                      9  \n",
       "3                     11  \n",
       "4                      8  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['english_sentence_length'] = train['english_sentence'].apply(lambda x: len(x.split()))\n",
    "train['hindi_sentence_length'] = train['hindi_sentence'].apply(lambda x: len(x.split()))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 👇1 ----> 3212 means it has 3212 english sentences of length=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     3212\n",
       "2     3730\n",
       "3     3862\n",
       "4     4981\n",
       "5     6090\n",
       "6     6912\n",
       "7     7214\n",
       "8     7184\n",
       "9     6714\n",
       "10    6132\n",
       "11    5608\n",
       "12    5047\n",
       "13    4338\n",
       "14    3863\n",
       "Name: english_sentence_length, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['english_sentence_length'].value_counts().sort_index()[:MAX_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=train['english_sentence_length'].value_counts().sort_index()[:MAX_WORDS].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 👇Keep rows if length of english sentence >= 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "      <th>english_sentence_length</th>\n",
       "      <th>hindi_sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43345</th>\n",
       "      <td>love and pleasure.</td>\n",
       "      <td>और आनंद चाहिए |</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104364</th>\n",
       "      <td>Arm chair position</td>\n",
       "      <td>आराम कुर्सी पोजीशन</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43261</th>\n",
       "      <td>In October 2010,</td>\n",
       "      <td>अक्टूबर २०१० की बात है,</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84878</th>\n",
       "      <td>Jaishanker prasad((In individuality)</td>\n",
       "      <td>जयशंकर प्रसाद (अभिव्यक्ति में)</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20752</th>\n",
       "      <td>of deconstructing, redefining,</td>\n",
       "      <td>शुरू करनी होगी,</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            english_sentence                  hindi_sentence  \\\n",
       "43345                     love and pleasure.                 और आनंद चाहिए |   \n",
       "104364                    Arm chair position              आराम कुर्सी पोजीशन   \n",
       "43261                       In October 2010,         अक्टूबर २०१० की बात है,   \n",
       "84878   Jaishanker prasad((In individuality)  जयशंकर प्रसाद (अभिव्यक्ति में)   \n",
       "20752         of deconstructing, redefining,                 शुरू करनी होगी,   \n",
       "\n",
       "        english_sentence_length  hindi_sentence_length  \n",
       "43345                         3                      4  \n",
       "104364                        3                      3  \n",
       "43261                         3                      5  \n",
       "84878                         3                      4  \n",
       "20752                         3                      3  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=train.sort_values(by='english_sentence_length')\n",
    "train=train.iloc[3212+3730+3862:x]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 👇1 ----> 3595 means it has 3595 hindi sentences of length=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     3595\n",
       "2     3367\n",
       "3     3123\n",
       "4     3718\n",
       "5     4595\n",
       "6     5658\n",
       "7     6184\n",
       "8     6323\n",
       "9     6374\n",
       "10    6134\n",
       "11    5509\n",
       "12    5214\n",
       "13    4673\n",
       "14    4112\n",
       "Name: hindi_sentence_length, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['hindi_sentence_length'].value_counts().sort_index()[:MAX_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=train['hindi_sentence_length'].value_counts().sort_index()[:MAX_WORDS].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 👇Keep rows if length of hindi sentence >= 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "      <th>english_sentence_length</th>\n",
       "      <th>hindi_sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115576</th>\n",
       "      <td>Dainik Jagaran Number 1.</td>\n",
       "      <td>दैनिक जागरण नम्बर १</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13750</th>\n",
       "      <td>Sick or disabled</td>\n",
       "      <td>बीमार या फिर विकलांग</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59202</th>\n",
       "      <td>It is written as follows:</td>\n",
       "      <td>वह अभिलेख निम्नलिखित है:</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34770</th>\n",
       "      <td>The contribution of mahadevi verma</td>\n",
       "      <td>महादेवी वर्मा का योगदान</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5164</th>\n",
       "      <td>integrating wireless networking</td>\n",
       "      <td>जीपीएस और जीएसएम को</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          english_sentence            hindi_sentence  \\\n",
       "115576            Dainik Jagaran Number 1.       दैनिक जागरण नम्बर १   \n",
       "13750                     Sick or disabled      बीमार या फिर विकलांग   \n",
       "59202            It is written as follows:  वह अभिलेख निम्नलिखित है:   \n",
       "34770   The contribution of mahadevi verma   महादेवी वर्मा का योगदान   \n",
       "5164       integrating wireless networking       जीपीएस और जीएसएम को   \n",
       "\n",
       "        english_sentence_length  hindi_sentence_length  \n",
       "115576                        4                      4  \n",
       "13750                         3                      4  \n",
       "59202                         5                      4  \n",
       "34770                         5                      4  \n",
       "5164                          3                      4  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=train.sort_values(by='hindi_sentence_length')\n",
    "train=train.iloc[3595+3367+3123:x]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples:  58494\n"
     ]
    }
   ],
   "source": [
    "print(\"Total samples: \",len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly sample and keep SAMPLES=50000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "      <th>english_sentence_length</th>\n",
       "      <th>hindi_sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87326</th>\n",
       "      <td>mumbai team is going to Rangy trophy on behalf...</td>\n",
       "      <td>मुंबई क्रिकेट टीम रणजी ट्रॉफी में शहर का प्रति...</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117649</th>\n",
       "      <td>from the punishments of my cancer,</td>\n",
       "      <td>मेरी कैंसर के दंड से,</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37737</th>\n",
       "      <td>I learned this firsthand with my next adventure.</td>\n",
       "      <td>यह मैंने खुद अपने अगले अनुभव से सीखा.</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90347</th>\n",
       "      <td>It's a great exercise</td>\n",
       "      <td>यह एक महान प्रयोग है</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119006</th>\n",
       "      <td>complexity in visual language</td>\n",
       "      <td>जटिलता होती है दृश्य भाषा की</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         english_sentence  \\\n",
       "87326   mumbai team is going to Rangy trophy on behalf...   \n",
       "117649                 from the punishments of my cancer,   \n",
       "37737    I learned this firsthand with my next adventure.   \n",
       "90347                               It's a great exercise   \n",
       "119006                      complexity in visual language   \n",
       "\n",
       "                                           hindi_sentence  \\\n",
       "87326   मुंबई क्रिकेट टीम रणजी ट्रॉफी में शहर का प्रति...   \n",
       "117649                              मेरी कैंसर के दंड से,   \n",
       "37737               यह मैंने खुद अपने अगले अनुभव से सीखा.   \n",
       "90347                                यह एक महान प्रयोग है   \n",
       "119006                       जटिलता होती है दृश्य भाषा की   \n",
       "\n",
       "        english_sentence_length  hindi_sentence_length  \n",
       "87326                        12                     11  \n",
       "117649                        6                      5  \n",
       "37737                         8                      8  \n",
       "90347                         4                      5  \n",
       "119006                        4                      6  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=train.sample(SAMPLES)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add <start\\>  and <end\\> token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "      <th>english_sentence_length</th>\n",
       "      <th>hindi_sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87326</th>\n",
       "      <td>&lt;start&gt; mumbai team is going to Rangy trophy o...</td>\n",
       "      <td>&lt;start&gt; मुंबई क्रिकेट टीम रणजी ट्रॉफी में शहर ...</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117649</th>\n",
       "      <td>&lt;start&gt; from the punishments of my cancer, &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; मेरी कैंसर के दंड से, &lt;end&gt;</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37737</th>\n",
       "      <td>&lt;start&gt; I learned this firsthand with my next ...</td>\n",
       "      <td>&lt;start&gt; यह मैंने खुद अपने अगले अनुभव से सीखा. ...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90347</th>\n",
       "      <td>&lt;start&gt; It's a great exercise &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; यह एक महान प्रयोग है &lt;end&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119006</th>\n",
       "      <td>&lt;start&gt; complexity in visual language &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; जटिलता होती है दृश्य भाषा की &lt;end&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         english_sentence  \\\n",
       "87326   <start> mumbai team is going to Rangy trophy o...   \n",
       "117649   <start> from the punishments of my cancer, <end>   \n",
       "37737   <start> I learned this firsthand with my next ...   \n",
       "90347                 <start> It's a great exercise <end>   \n",
       "119006        <start> complexity in visual language <end>   \n",
       "\n",
       "                                           hindi_sentence  \\\n",
       "87326   <start> मुंबई क्रिकेट टीम रणजी ट्रॉफी में शहर ...   \n",
       "117649                <start> मेरी कैंसर के दंड से, <end>   \n",
       "37737   <start> यह मैंने खुद अपने अगले अनुभव से सीखा. ...   \n",
       "90347                  <start> यह एक महान प्रयोग है <end>   \n",
       "119006         <start> जटिलता होती है दृश्य भाषा की <end>   \n",
       "\n",
       "        english_sentence_length  hindi_sentence_length  \n",
       "87326                        12                     11  \n",
       "117649                        6                      5  \n",
       "37737                         8                      8  \n",
       "90347                         4                      5  \n",
       "119006                        4                      6  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['english_sentence'] = train['english_sentence'].apply(lambda x: '<start> '+str(' '.join(x.split()[:MAX_WORDS]))+' <end>')\n",
    "train['hindi_sentence'] = train['hindi_sentence'].apply(lambda x: '<start> '+str(' '.join(x.split()[:MAX_WORDS]))+' <end>')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS=MAX_WORDS+2 #add 2 for <start>,<end> token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and pad english sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizerE = Tokenizer(num_words=VOCAB, \n",
    "                       oov_token='<OOV>', \n",
    "                       lower=True, \n",
    "                       filters='#$!\"%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "\n",
    "tokenizerE.fit_on_texts(train['english_sentence'])\n",
    "\n",
    "eng_inp = tokenizerE.texts_to_sequences(train['english_sentence'])\n",
    "\n",
    "eng_inp = pad_sequences(eng_inp,\n",
    "                        maxlen=MAX_WORDS,\n",
    "                        truncating='post',\n",
    "                        padding='post',\n",
    "                        dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and pad hindi sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizerH = Tokenizer(num_words=VOCAB,\n",
    "                       oov_token='<OOV>',\n",
    "                       lower=True,\n",
    "                       filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "\n",
    "tokenizerH.fit_on_texts(train['hindi_sentence'])\n",
    "\n",
    "hin_inp=tokenizerH.texts_to_sequences(train['hindi_sentence'])\n",
    "\n",
    "hin_inp=pad_sequences(hin_inp,\n",
    "                      maxlen=MAX_WORDS,\n",
    "                      truncating='post',\n",
    "                      padding='post',\n",
    "                      dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating reverse hindi dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_hin_dict = dict(map(reversed, tokenizerH.word_index.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating datatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(eng_inp, hin_inp, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(BATCH_SIZE,drop_remainder=True)\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):    \n",
    "    def __init__(self,vocab,batch_size):\n",
    "        super(Encoder,self).__init__()  \n",
    "        self.vocab=vocab\n",
    "        self.batch_size=batch_size\n",
    "        self.embed=Embedding(self.vocab,256)\n",
    "        self.gru=Bidirectional(GRU(256,return_state=True,return_sequences=True,\n",
    "                                   recurrent_initializer='glorot_uniform',dropout=0.5))\n",
    "        self.gru1=Bidirectional(GRU(256,return_state=True,return_sequences=True,\n",
    "                                   recurrent_initializer='glorot_uniform',dropout=0.5))\n",
    "    \n",
    "    def call(self,encoder_inp,hidden):\n",
    "        encoder_inp=self.embed(encoder_inp)       \n",
    "        _,state_htmp,state_ctmp=self.gru(encoder_inp,initial_state=hidden)\n",
    "        encoder_out,state_h,state_c=self.gru1(encoder_inp,initial_state=[state_htmp,state_ctmp])\n",
    "        return encoder_out,tf.concat([state_h,state_c],axis=1)\n",
    "    \n",
    "    def initialise_hidden_unit(self):\n",
    "        return [tf.zeros((self.batch_size,256)) for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=Encoder(VOCAB,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):   \n",
    "    def __init__(self,vocab):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.vocab=vocab\n",
    "        self.embed=Embedding(self.vocab,256)\n",
    "        self.dense=Dense(512)\n",
    "        self.dense1=Dense(512)\n",
    "        self.dense2=Dense(1)\n",
    "        self.gru=GRU(512,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform',\n",
    "                     dropout=0.5)\n",
    "        self.dense3=Dense(self.vocab)\n",
    "        \n",
    "    def call(self,decoder_inp,encoder_out,carry):       \n",
    "        decoder_inp=self.embed(decoder_inp)\n",
    "        carry=tf.expand_dims(carry,1)\n",
    "#----------------------------------------------------------------\n",
    "        #attention\n",
    "        score=self.dense2(tf.math.tanh(self.dense1(encoder_out)+self.dense(carry)))\n",
    "        attention_weights=tf.nn.softmax(score,axis=1)\n",
    "        context_vector=tf.math.reduce_sum(attention_weights*encoder_out,axis=1,keepdims=True)\n",
    "        merged_vector=tf.concat([context_vector,decoder_inp],axis=-1)\n",
    "#-----------------------------------------------------------------        \n",
    "        decoder_out,decoder_state=self.gru(merged_vector)\n",
    "        \n",
    "        decoder_out=tf.reshape(decoder_out,(-1,decoder_out.shape[2]))\n",
    "        decoder_out=self.dense3(decoder_out)\n",
    "        return decoder_out,decoder_state,attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder=Decoder(VOCAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoint_dir = CHECKPOINT_PATH\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train(inp,out,hidden,MAX_WORDS):\n",
    "    loss=0\n",
    "    with tf.GradientTape() as tape:\n",
    "        eo,hidden=encoder(inp,hidden)\n",
    "        h=hidden       \n",
    "        hi=tf.expand_dims([tokenizerH.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "        hi=tf.cast(hi,'float32')        \n",
    "        for i in range (1,MAX_WORDS):           \n",
    "            do,ds,_=decoder(hi,eo,h) \n",
    "            loss+=loss_function(out[:, i], do)\n",
    "            hi=tf.expand_dims(out[:, i], 1) \n",
    "            hi=tf.cast(hi,'float32')    \n",
    "    variables = encoder.trainable_variables+decoder.trainable_variables\n",
    "    gradients=tape.gradient(loss, variables) \n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    batch_loss = loss / MAX_WORDS\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test(inp_,out_,hidden_,MAX_WORDS):\n",
    "    loss_=0\n",
    "    eo_,hidden_=encoder(inp_,hidden_)\n",
    "    h_=hidden_       \n",
    "    hi_=tf.expand_dims([tokenizerH.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "    hi_=tf.cast(hi_,'float32')        \n",
    "    for i in range (1,MAX_WORDS):           \n",
    "        do_,ds_,_=decoder(hi_,eo_,h_) \n",
    "        loss_+=loss_function(out_[:, i], do_)\n",
    "        hi_=tf.expand_dims(out_[:, i], 1) \n",
    "        hi_=tf.cast(hi_,'float32')    \n",
    "    batch_loss_ = loss_ / MAX_WORDS\n",
    "    return batch_loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the training and testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Train Loss = 4.0092\tVal Loss = 3.7056\tTime taken = 105.73 secs\n",
      "Starting epoch 2\n",
      "Train Loss = 3.5157\tVal Loss = 3.4562\tTime taken = 92.80 secs\n",
      "Starting epoch 3\n",
      "Train Loss = 3.2791\tVal Loss = 3.2927\tTime taken = 92.61 secs\n",
      "Starting epoch 4\n",
      "Train Loss = 3.0564\tVal Loss = 3.1344\tTime taken = 91.73 secs\n",
      "Starting epoch 5\n",
      "Train Loss = 2.8119\tVal Loss = 2.9908\tTime taken = 92.05 secs\n",
      "Starting epoch 6\n",
      "Train Loss = 2.5528\tVal Loss = 2.8794\tTime taken = 92.03 secs\n",
      "Starting epoch 7\n",
      "Train Loss = 2.3121\tVal Loss = 2.8070\tTime taken = 92.22 secs\n",
      "Starting epoch 8\n",
      "Train Loss = 2.0998\tVal Loss = 2.7641\tTime taken = 91.99 secs\n",
      "Starting epoch 9\n",
      "Train Loss = 1.9153\tVal Loss = 2.7490\tTime taken = 91.97 secs\n",
      "Starting epoch 10\n",
      "Train Loss = 1.7522\tVal Loss = 2.7428\tTime taken = 91.91 secs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for epoch in range(10):   \n",
    "    print(\"Starting epoch {}\".format(epoch+1))\n",
    "    start=time.time()  \n",
    "    \n",
    "    hidden = encoder.initialise_hidden_unit()\n",
    "    total_loss = 0  \n",
    "    \n",
    "    hidden_ = encoder.initialise_hidden_unit()\n",
    "    total_loss_ = 0 \n",
    "#---------------------------------------------------------------------------------------------- \n",
    "    for x_batch,y_batch in dataset_train: \n",
    "        batch_loss = train(x_batch, y_batch, hidden,MAX_WORDS) #calling the training loop\n",
    "        total_loss += batch_loss\n",
    "#----------------------------------------------------------------------------------------------\n",
    "    for x_batch,y_batch in dataset_test: \n",
    "        batch_loss_ = test(x_batch, y_batch, hidden_,MAX_WORDS) #calling the testing loop\n",
    "        total_loss_ += batch_loss_\n",
    "#----------------------------------------------------------------------------------------------\n",
    "        \n",
    "    end=time.time()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    print('Train Loss = {:.4f}\\tVal Loss = {:.4f}\\tTime taken = {:.2f} secs'.format(total_loss/(SAMPLES*0.9//BATCH_SIZE),\n",
    "                                                                                    total_loss_/(SAMPLES*0.1//BATCH_SIZE),\n",
    "                                                                                    (end-start))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>TESTING</f>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test,plot=False):\n",
    "    \n",
    "    test=' '.join(test.split()[:MAX_WORDS-2])\n",
    "    test=\"<start> \"+test+\" <end>\"\n",
    "    test_ori=test\n",
    "    \n",
    "    test=tokenizerE.texts_to_sequences([test])\n",
    "    test=pad_sequences(test,\n",
    "                       maxlen=MAX_WORDS,\n",
    "                       truncating='post',\n",
    "                       padding='post',\n",
    "                       dtype='float32')\n",
    "\n",
    "    hidd=[tf.zeros((1,256)) for i in range(2)]\n",
    "    testenou,testenhi=encoder(test,hidd)\n",
    "    testhi=tf.expand_dims([tokenizerH.word_index['<start>']] * 1, 1)\n",
    "    testhi=tf.cast(testhi,'float32')\n",
    "    pred=''\n",
    "\n",
    "    apn_att_wt=np.zeros((MAX_WORDS-1,MAX_WORDS))\n",
    "    for i in range (1,MAX_WORDS):           \n",
    "        testdo,testds,att_wt=decoder(testhi,testenou,testenhi)    \n",
    "        apn_att_wt[i-1]=tf.reshape(att_wt[0],(MAX_WORDS))\n",
    "        m=tf.math.argmax(testdo[0])\n",
    "\n",
    "        pred+=rev_hin_dict[m.numpy()]+' '\n",
    "        testhi=tf.expand_dims([m.numpy()] * 1, 1)\n",
    "        testhi=tf.cast(testhi,'float32')\n",
    "        testenhi=testds\n",
    "\n",
    "    pre=''\n",
    "    for word in pred.split():\n",
    "        if word=='<end>':\n",
    "            break\n",
    "        else:\n",
    "            pre+=word+' '\n",
    "\n",
    "    if plot==True:       \n",
    "        attention=np.reshape(apn_att_wt,(MAX_WORDS,MAX_WORDS-1))\n",
    "        fig = plt.figure(figsize=(5,5))\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        ax.matshow(attention, cmap='viridis')\n",
    "        plt.show()         \n",
    "\n",
    "    print(f\"INPUT  =  {test_ori}\\nHINDI  =  {pre}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT  =  <start> In jail, Bhagat singh and his friends were on hunger strike for 64 days. <end>\n",
      "HINDI  =  जेल में भगत सिंह और बाकि साथियो ने ६४ दिनो तक भूख हद्ताल कि। \n"
     ]
    }
   ],
   "source": [
    "evaluate(\"In jail, Bhagat singh and his friends were on hunger strike for 64 days.\", plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
