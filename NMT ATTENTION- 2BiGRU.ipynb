{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense,GRU,Bidirectional,Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>TRAINING</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH='data.csv'\n",
    "CHECKPOINT_PATH='training_checkpoints'\n",
    "\n",
    "MAX_WORDS=14\n",
    "SAMPLES=50000\n",
    "VOCAB=15000\n",
    "BATCH_SIZE=128\n",
    "EPOCH=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>‡§∞‡§æ‡§ú‡§®‡•Ä‡§§‡§ø‡§ú‡•ç‡§û‡•ã‡§Ç ‡§ï‡•á ‡§™‡§æ‡§∏ ‡§ú‡•ã ‡§ï‡§æ‡§∞‡•ç‡§Ø ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è, ‡§µ‡§π ‡§ï‡§∞...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'd like to tell you about one such child,</td>\n",
       "      <td>‡§Æ‡§à ‡§Ü‡§™‡§ï‡•ã ‡§ê‡§∏‡•á ‡§π‡•Ä ‡§è‡§ï ‡§¨‡§ö‡•ç‡§ö‡•á ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§®‡§æ ‡§ö‡§æ‡§π‡•Ç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This percentage is even greater than the perce...</td>\n",
       "      <td>‡§Ø‡§π ‡§™‡•ç‡§∞‡§§‡§ø‡§∂‡§§ ‡§≠‡§æ‡§∞‡§§ ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§®‡•ç‡§¶‡•Å‡§ì‡§Ç ‡§™‡•ç‡§∞‡§§‡§ø‡§∂‡§§ ‡§∏‡•á ‡§Ö‡§ß‡§ø‡§ï ‡§π‡•à‡•§</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what we really mean is that they're bad at not...</td>\n",
       "      <td>‡§π‡§Æ ‡§Ø‡•á ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•á ‡§ï‡§ø ‡§µ‡•ã ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§®‡§π‡•Ä‡§Ç ‡§¶‡•á ‡§™‡§æ‡§§‡•á</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.The ending portion of these Vedas is called U...</td>\n",
       "      <td>‡§á‡§®‡•ç‡§π‡•Ä‡§Ç ‡§µ‡•á‡§¶‡•ã‡§Ç ‡§ï‡§æ ‡§Ö‡§Ç‡§§‡§ø‡§Æ ‡§≠‡§æ‡§ó ‡§â‡§™‡§®‡§ø‡§∑‡§¶ ‡§ï‡§π‡§≤‡§æ‡§§‡§æ ‡§π‡•à‡•§</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    english_sentence  \\\n",
       "0  politicians do not have permission to do what ...   \n",
       "1         I'd like to tell you about one such child,   \n",
       "2  This percentage is even greater than the perce...   \n",
       "3  what we really mean is that they're bad at not...   \n",
       "4  .The ending portion of these Vedas is called U...   \n",
       "\n",
       "                                      hindi_sentence  \n",
       "0  ‡§∞‡§æ‡§ú‡§®‡•Ä‡§§‡§ø‡§ú‡•ç‡§û‡•ã‡§Ç ‡§ï‡•á ‡§™‡§æ‡§∏ ‡§ú‡•ã ‡§ï‡§æ‡§∞‡•ç‡§Ø ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è, ‡§µ‡§π ‡§ï‡§∞...  \n",
       "1  ‡§Æ‡§à ‡§Ü‡§™‡§ï‡•ã ‡§ê‡§∏‡•á ‡§π‡•Ä ‡§è‡§ï ‡§¨‡§ö‡•ç‡§ö‡•á ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§®‡§æ ‡§ö‡§æ‡§π‡•Ç...  \n",
       "2   ‡§Ø‡§π ‡§™‡•ç‡§∞‡§§‡§ø‡§∂‡§§ ‡§≠‡§æ‡§∞‡§§ ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§®‡•ç‡§¶‡•Å‡§ì‡§Ç ‡§™‡•ç‡§∞‡§§‡§ø‡§∂‡§§ ‡§∏‡•á ‡§Ö‡§ß‡§ø‡§ï ‡§π‡•à‡•§  \n",
       "3     ‡§π‡§Æ ‡§Ø‡•á ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•á ‡§ï‡§ø ‡§µ‡•ã ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§®‡§π‡•Ä‡§Ç ‡§¶‡•á ‡§™‡§æ‡§§‡•á  \n",
       "4        ‡§á‡§®‡•ç‡§π‡•Ä‡§Ç ‡§µ‡•á‡§¶‡•ã‡§Ç ‡§ï‡§æ ‡§Ö‡§Ç‡§§‡§ø‡§Æ ‡§≠‡§æ‡§ó ‡§â‡§™‡§®‡§ø‡§∑‡§¶ ‡§ï‡§π‡§≤‡§æ‡§§‡§æ ‡§π‡•à‡•§  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=pd.read_csv(DATA_PATH)\n",
    "train=train.drop(\"source\",axis=1)\n",
    "train=train.dropna() #drop NA rows\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Samples:  127605\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Samples: \",len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count length of english and hindi sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "      <th>english_sentence_length</th>\n",
       "      <th>hindi_sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>‡§∞‡§æ‡§ú‡§®‡•Ä‡§§‡§ø‡§ú‡•ç‡§û‡•ã‡§Ç ‡§ï‡•á ‡§™‡§æ‡§∏ ‡§ú‡•ã ‡§ï‡§æ‡§∞‡•ç‡§Ø ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è, ‡§µ‡§π ‡§ï‡§∞...</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'd like to tell you about one such child,</td>\n",
       "      <td>‡§Æ‡§à ‡§Ü‡§™‡§ï‡•ã ‡§ê‡§∏‡•á ‡§π‡•Ä ‡§è‡§ï ‡§¨‡§ö‡•ç‡§ö‡•á ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§®‡§æ ‡§ö‡§æ‡§π‡•Ç...</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This percentage is even greater than the perce...</td>\n",
       "      <td>‡§Ø‡§π ‡§™‡•ç‡§∞‡§§‡§ø‡§∂‡§§ ‡§≠‡§æ‡§∞‡§§ ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§®‡•ç‡§¶‡•Å‡§ì‡§Ç ‡§™‡•ç‡§∞‡§§‡§ø‡§∂‡§§ ‡§∏‡•á ‡§Ö‡§ß‡§ø‡§ï ‡§π‡•à‡•§</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what we really mean is that they're bad at not...</td>\n",
       "      <td>‡§π‡§Æ ‡§Ø‡•á ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•á ‡§ï‡§ø ‡§µ‡•ã ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§®‡§π‡•Ä‡§Ç ‡§¶‡•á ‡§™‡§æ‡§§‡•á</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.The ending portion of these Vedas is called U...</td>\n",
       "      <td>‡§á‡§®‡•ç‡§π‡•Ä‡§Ç ‡§µ‡•á‡§¶‡•ã‡§Ç ‡§ï‡§æ ‡§Ö‡§Ç‡§§‡§ø‡§Æ ‡§≠‡§æ‡§ó ‡§â‡§™‡§®‡§ø‡§∑‡§¶ ‡§ï‡§π‡§≤‡§æ‡§§‡§æ ‡§π‡•à‡•§</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    english_sentence  \\\n",
       "0  politicians do not have permission to do what ...   \n",
       "1         I'd like to tell you about one such child,   \n",
       "2  This percentage is even greater than the perce...   \n",
       "3  what we really mean is that they're bad at not...   \n",
       "4  .The ending portion of these Vedas is called U...   \n",
       "\n",
       "                                      hindi_sentence  english_sentence_length  \\\n",
       "0  ‡§∞‡§æ‡§ú‡§®‡•Ä‡§§‡§ø‡§ú‡•ç‡§û‡•ã‡§Ç ‡§ï‡•á ‡§™‡§æ‡§∏ ‡§ú‡•ã ‡§ï‡§æ‡§∞‡•ç‡§Ø ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è, ‡§µ‡§π ‡§ï‡§∞...                       12   \n",
       "1  ‡§Æ‡§à ‡§Ü‡§™‡§ï‡•ã ‡§ê‡§∏‡•á ‡§π‡•Ä ‡§è‡§ï ‡§¨‡§ö‡•ç‡§ö‡•á ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§®‡§æ ‡§ö‡§æ‡§π‡•Ç...                        9   \n",
       "2   ‡§Ø‡§π ‡§™‡•ç‡§∞‡§§‡§ø‡§∂‡§§ ‡§≠‡§æ‡§∞‡§§ ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§®‡•ç‡§¶‡•Å‡§ì‡§Ç ‡§™‡•ç‡§∞‡§§‡§ø‡§∂‡§§ ‡§∏‡•á ‡§Ö‡§ß‡§ø‡§ï ‡§π‡•à‡•§                       10   \n",
       "3     ‡§π‡§Æ ‡§Ø‡•á ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•á ‡§ï‡§ø ‡§µ‡•ã ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§®‡§π‡•Ä‡§Ç ‡§¶‡•á ‡§™‡§æ‡§§‡•á                       12   \n",
       "4        ‡§á‡§®‡•ç‡§π‡•Ä‡§Ç ‡§µ‡•á‡§¶‡•ã‡§Ç ‡§ï‡§æ ‡§Ö‡§Ç‡§§‡§ø‡§Æ ‡§≠‡§æ‡§ó ‡§â‡§™‡§®‡§ø‡§∑‡§¶ ‡§ï‡§π‡§≤‡§æ‡§§‡§æ ‡§π‡•à‡•§                        9   \n",
       "\n",
       "   hindi_sentence_length  \n",
       "0                     14  \n",
       "1                     11  \n",
       "2                      9  \n",
       "3                     11  \n",
       "4                      8  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['english_sentence_length'] = train['english_sentence'].apply(lambda x: len(x.split()))\n",
    "train['hindi_sentence_length'] = train['hindi_sentence'].apply(lambda x: len(x.split()))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëá1 ----> 3212 means it has 3212 english sentences of length=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     3212\n",
       "2     3730\n",
       "3     3862\n",
       "4     4981\n",
       "5     6090\n",
       "6     6912\n",
       "7     7214\n",
       "8     7184\n",
       "9     6714\n",
       "10    6132\n",
       "11    5608\n",
       "12    5047\n",
       "13    4338\n",
       "14    3863\n",
       "Name: english_sentence_length, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['english_sentence_length'].value_counts().sort_index()[:MAX_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=train['english_sentence_length'].value_counts().sort_index()[:MAX_WORDS].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëáKeep rows if length of english sentence >= 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "      <th>english_sentence_length</th>\n",
       "      <th>hindi_sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43345</th>\n",
       "      <td>love and pleasure.</td>\n",
       "      <td>‡§î‡§∞ ‡§Ü‡§®‡§Ç‡§¶ ‡§ö‡§æ‡§π‡§ø‡§è |</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104364</th>\n",
       "      <td>Arm chair position</td>\n",
       "      <td>‡§Ü‡§∞‡§æ‡§Æ ‡§ï‡•Å‡§∞‡•ç‡§∏‡•Ä ‡§™‡•ã‡§ú‡•Ä‡§∂‡§®</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43261</th>\n",
       "      <td>In October 2010,</td>\n",
       "      <td>‡§Ö‡§ï‡•ç‡§ü‡•Ç‡§¨‡§∞ ‡•®‡•¶‡•ß‡•¶ ‡§ï‡•Ä ‡§¨‡§æ‡§§ ‡§π‡•à,</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84878</th>\n",
       "      <td>Jaishanker prasad((In individuality)</td>\n",
       "      <td>‡§ú‡§Ø‡§∂‡§Ç‡§ï‡§∞ ‡§™‡•ç‡§∞‡§∏‡§æ‡§¶ (‡§Ö‡§≠‡§ø‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø ‡§Æ‡•á‡§Ç)</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20752</th>\n",
       "      <td>of deconstructing, redefining,</td>\n",
       "      <td>‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞‡§®‡•Ä ‡§π‡•ã‡§ó‡•Ä,</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            english_sentence                  hindi_sentence  \\\n",
       "43345                     love and pleasure.                 ‡§î‡§∞ ‡§Ü‡§®‡§Ç‡§¶ ‡§ö‡§æ‡§π‡§ø‡§è |   \n",
       "104364                    Arm chair position              ‡§Ü‡§∞‡§æ‡§Æ ‡§ï‡•Å‡§∞‡•ç‡§∏‡•Ä ‡§™‡•ã‡§ú‡•Ä‡§∂‡§®   \n",
       "43261                       In October 2010,         ‡§Ö‡§ï‡•ç‡§ü‡•Ç‡§¨‡§∞ ‡•®‡•¶‡•ß‡•¶ ‡§ï‡•Ä ‡§¨‡§æ‡§§ ‡§π‡•à,   \n",
       "84878   Jaishanker prasad((In individuality)  ‡§ú‡§Ø‡§∂‡§Ç‡§ï‡§∞ ‡§™‡•ç‡§∞‡§∏‡§æ‡§¶ (‡§Ö‡§≠‡§ø‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø ‡§Æ‡•á‡§Ç)   \n",
       "20752         of deconstructing, redefining,                 ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞‡§®‡•Ä ‡§π‡•ã‡§ó‡•Ä,   \n",
       "\n",
       "        english_sentence_length  hindi_sentence_length  \n",
       "43345                         3                      4  \n",
       "104364                        3                      3  \n",
       "43261                         3                      5  \n",
       "84878                         3                      4  \n",
       "20752                         3                      3  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=train.sort_values(by='english_sentence_length')\n",
    "train=train.iloc[3212+3730+3862:x]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëá1 ----> 3595 means it has 3595 hindi sentences of length=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     3595\n",
       "2     3367\n",
       "3     3123\n",
       "4     3718\n",
       "5     4595\n",
       "6     5658\n",
       "7     6184\n",
       "8     6323\n",
       "9     6374\n",
       "10    6134\n",
       "11    5509\n",
       "12    5214\n",
       "13    4673\n",
       "14    4112\n",
       "Name: hindi_sentence_length, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['hindi_sentence_length'].value_counts().sort_index()[:MAX_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=train['hindi_sentence_length'].value_counts().sort_index()[:MAX_WORDS].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëáKeep rows if length of hindi sentence >= 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "      <th>english_sentence_length</th>\n",
       "      <th>hindi_sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115576</th>\n",
       "      <td>Dainik Jagaran Number 1.</td>\n",
       "      <td>‡§¶‡•à‡§®‡§ø‡§ï ‡§ú‡§æ‡§ó‡§∞‡§£ ‡§®‡§Æ‡•ç‡§¨‡§∞ ‡•ß</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13750</th>\n",
       "      <td>Sick or disabled</td>\n",
       "      <td>‡§¨‡•Ä‡§Æ‡§æ‡§∞ ‡§Ø‡§æ ‡§´‡§ø‡§∞ ‡§µ‡§ø‡§ï‡§≤‡§æ‡§Ç‡§ó</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59202</th>\n",
       "      <td>It is written as follows:</td>\n",
       "      <td>‡§µ‡§π ‡§Ö‡§≠‡§ø‡§≤‡•á‡§ñ ‡§®‡§ø‡§Æ‡•ç‡§®‡§≤‡§ø‡§ñ‡§ø‡§§ ‡§π‡•à:</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34770</th>\n",
       "      <td>The contribution of mahadevi verma</td>\n",
       "      <td>‡§Æ‡§π‡§æ‡§¶‡•á‡§µ‡•Ä ‡§µ‡§∞‡•ç‡§Æ‡§æ ‡§ï‡§æ ‡§Ø‡•ã‡§ó‡§¶‡§æ‡§®</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5164</th>\n",
       "      <td>integrating wireless networking</td>\n",
       "      <td>‡§ú‡•Ä‡§™‡•Ä‡§è‡§∏ ‡§î‡§∞ ‡§ú‡•Ä‡§è‡§∏‡§è‡§Æ ‡§ï‡•ã</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          english_sentence            hindi_sentence  \\\n",
       "115576            Dainik Jagaran Number 1.       ‡§¶‡•à‡§®‡§ø‡§ï ‡§ú‡§æ‡§ó‡§∞‡§£ ‡§®‡§Æ‡•ç‡§¨‡§∞ ‡•ß   \n",
       "13750                     Sick or disabled      ‡§¨‡•Ä‡§Æ‡§æ‡§∞ ‡§Ø‡§æ ‡§´‡§ø‡§∞ ‡§µ‡§ø‡§ï‡§≤‡§æ‡§Ç‡§ó   \n",
       "59202            It is written as follows:  ‡§µ‡§π ‡§Ö‡§≠‡§ø‡§≤‡•á‡§ñ ‡§®‡§ø‡§Æ‡•ç‡§®‡§≤‡§ø‡§ñ‡§ø‡§§ ‡§π‡•à:   \n",
       "34770   The contribution of mahadevi verma   ‡§Æ‡§π‡§æ‡§¶‡•á‡§µ‡•Ä ‡§µ‡§∞‡•ç‡§Æ‡§æ ‡§ï‡§æ ‡§Ø‡•ã‡§ó‡§¶‡§æ‡§®   \n",
       "5164       integrating wireless networking       ‡§ú‡•Ä‡§™‡•Ä‡§è‡§∏ ‡§î‡§∞ ‡§ú‡•Ä‡§è‡§∏‡§è‡§Æ ‡§ï‡•ã   \n",
       "\n",
       "        english_sentence_length  hindi_sentence_length  \n",
       "115576                        4                      4  \n",
       "13750                         3                      4  \n",
       "59202                         5                      4  \n",
       "34770                         5                      4  \n",
       "5164                          3                      4  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=train.sort_values(by='hindi_sentence_length')\n",
    "train=train.iloc[3595+3367+3123:x]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples:  58494\n"
     ]
    }
   ],
   "source": [
    "print(\"Total samples: \",len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly sample and keep SAMPLES=50000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "      <th>english_sentence_length</th>\n",
       "      <th>hindi_sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87326</th>\n",
       "      <td>mumbai team is going to Rangy trophy on behalf...</td>\n",
       "      <td>‡§Æ‡•Å‡§Ç‡§¨‡§à ‡§ï‡•ç‡§∞‡§ø‡§ï‡•á‡§ü ‡§ü‡•Ä‡§Æ ‡§∞‡§£‡§ú‡•Ä ‡§ü‡•ç‡§∞‡•â‡§´‡•Ä ‡§Æ‡•á‡§Ç ‡§∂‡§π‡§∞ ‡§ï‡§æ ‡§™‡•ç‡§∞‡§§‡§ø...</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117649</th>\n",
       "      <td>from the punishments of my cancer,</td>\n",
       "      <td>‡§Æ‡•á‡§∞‡•Ä ‡§ï‡•à‡§Ç‡§∏‡§∞ ‡§ï‡•á ‡§¶‡§Ç‡§° ‡§∏‡•á,</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37737</th>\n",
       "      <td>I learned this firsthand with my next adventure.</td>\n",
       "      <td>‡§Ø‡§π ‡§Æ‡•à‡§Ç‡§®‡•á ‡§ñ‡•Å‡§¶ ‡§Ö‡§™‡§®‡•á ‡§Ö‡§ó‡§≤‡•á ‡§Ö‡§®‡•Å‡§≠‡§µ ‡§∏‡•á ‡§∏‡•Ä‡§ñ‡§æ.</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90347</th>\n",
       "      <td>It's a great exercise</td>\n",
       "      <td>‡§Ø‡§π ‡§è‡§ï ‡§Æ‡§π‡§æ‡§® ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§π‡•à</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119006</th>\n",
       "      <td>complexity in visual language</td>\n",
       "      <td>‡§ú‡§ü‡§ø‡§≤‡§§‡§æ ‡§π‡•ã‡§§‡•Ä ‡§π‡•à ‡§¶‡•É‡§∂‡•ç‡§Ø ‡§≠‡§æ‡§∑‡§æ ‡§ï‡•Ä</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         english_sentence  \\\n",
       "87326   mumbai team is going to Rangy trophy on behalf...   \n",
       "117649                 from the punishments of my cancer,   \n",
       "37737    I learned this firsthand with my next adventure.   \n",
       "90347                               It's a great exercise   \n",
       "119006                      complexity in visual language   \n",
       "\n",
       "                                           hindi_sentence  \\\n",
       "87326   ‡§Æ‡•Å‡§Ç‡§¨‡§à ‡§ï‡•ç‡§∞‡§ø‡§ï‡•á‡§ü ‡§ü‡•Ä‡§Æ ‡§∞‡§£‡§ú‡•Ä ‡§ü‡•ç‡§∞‡•â‡§´‡•Ä ‡§Æ‡•á‡§Ç ‡§∂‡§π‡§∞ ‡§ï‡§æ ‡§™‡•ç‡§∞‡§§‡§ø...   \n",
       "117649                              ‡§Æ‡•á‡§∞‡•Ä ‡§ï‡•à‡§Ç‡§∏‡§∞ ‡§ï‡•á ‡§¶‡§Ç‡§° ‡§∏‡•á,   \n",
       "37737               ‡§Ø‡§π ‡§Æ‡•à‡§Ç‡§®‡•á ‡§ñ‡•Å‡§¶ ‡§Ö‡§™‡§®‡•á ‡§Ö‡§ó‡§≤‡•á ‡§Ö‡§®‡•Å‡§≠‡§µ ‡§∏‡•á ‡§∏‡•Ä‡§ñ‡§æ.   \n",
       "90347                                ‡§Ø‡§π ‡§è‡§ï ‡§Æ‡§π‡§æ‡§® ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§π‡•à   \n",
       "119006                       ‡§ú‡§ü‡§ø‡§≤‡§§‡§æ ‡§π‡•ã‡§§‡•Ä ‡§π‡•à ‡§¶‡•É‡§∂‡•ç‡§Ø ‡§≠‡§æ‡§∑‡§æ ‡§ï‡•Ä   \n",
       "\n",
       "        english_sentence_length  hindi_sentence_length  \n",
       "87326                        12                     11  \n",
       "117649                        6                      5  \n",
       "37737                         8                      8  \n",
       "90347                         4                      5  \n",
       "119006                        4                      6  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=train.sample(SAMPLES)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add <start\\>  and <end\\> token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "      <th>english_sentence_length</th>\n",
       "      <th>hindi_sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87326</th>\n",
       "      <td>&lt;start&gt; mumbai team is going to Rangy trophy o...</td>\n",
       "      <td>&lt;start&gt; ‡§Æ‡•Å‡§Ç‡§¨‡§à ‡§ï‡•ç‡§∞‡§ø‡§ï‡•á‡§ü ‡§ü‡•Ä‡§Æ ‡§∞‡§£‡§ú‡•Ä ‡§ü‡•ç‡§∞‡•â‡§´‡•Ä ‡§Æ‡•á‡§Ç ‡§∂‡§π‡§∞ ...</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117649</th>\n",
       "      <td>&lt;start&gt; from the punishments of my cancer, &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; ‡§Æ‡•á‡§∞‡•Ä ‡§ï‡•à‡§Ç‡§∏‡§∞ ‡§ï‡•á ‡§¶‡§Ç‡§° ‡§∏‡•á, &lt;end&gt;</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37737</th>\n",
       "      <td>&lt;start&gt; I learned this firsthand with my next ...</td>\n",
       "      <td>&lt;start&gt; ‡§Ø‡§π ‡§Æ‡•à‡§Ç‡§®‡•á ‡§ñ‡•Å‡§¶ ‡§Ö‡§™‡§®‡•á ‡§Ö‡§ó‡§≤‡•á ‡§Ö‡§®‡•Å‡§≠‡§µ ‡§∏‡•á ‡§∏‡•Ä‡§ñ‡§æ. ...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90347</th>\n",
       "      <td>&lt;start&gt; It's a great exercise &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; ‡§Ø‡§π ‡§è‡§ï ‡§Æ‡§π‡§æ‡§® ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§π‡•à &lt;end&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119006</th>\n",
       "      <td>&lt;start&gt; complexity in visual language &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; ‡§ú‡§ü‡§ø‡§≤‡§§‡§æ ‡§π‡•ã‡§§‡•Ä ‡§π‡•à ‡§¶‡•É‡§∂‡•ç‡§Ø ‡§≠‡§æ‡§∑‡§æ ‡§ï‡•Ä &lt;end&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         english_sentence  \\\n",
       "87326   <start> mumbai team is going to Rangy trophy o...   \n",
       "117649   <start> from the punishments of my cancer, <end>   \n",
       "37737   <start> I learned this firsthand with my next ...   \n",
       "90347                 <start> It's a great exercise <end>   \n",
       "119006        <start> complexity in visual language <end>   \n",
       "\n",
       "                                           hindi_sentence  \\\n",
       "87326   <start> ‡§Æ‡•Å‡§Ç‡§¨‡§à ‡§ï‡•ç‡§∞‡§ø‡§ï‡•á‡§ü ‡§ü‡•Ä‡§Æ ‡§∞‡§£‡§ú‡•Ä ‡§ü‡•ç‡§∞‡•â‡§´‡•Ä ‡§Æ‡•á‡§Ç ‡§∂‡§π‡§∞ ...   \n",
       "117649                <start> ‡§Æ‡•á‡§∞‡•Ä ‡§ï‡•à‡§Ç‡§∏‡§∞ ‡§ï‡•á ‡§¶‡§Ç‡§° ‡§∏‡•á, <end>   \n",
       "37737   <start> ‡§Ø‡§π ‡§Æ‡•à‡§Ç‡§®‡•á ‡§ñ‡•Å‡§¶ ‡§Ö‡§™‡§®‡•á ‡§Ö‡§ó‡§≤‡•á ‡§Ö‡§®‡•Å‡§≠‡§µ ‡§∏‡•á ‡§∏‡•Ä‡§ñ‡§æ. ...   \n",
       "90347                  <start> ‡§Ø‡§π ‡§è‡§ï ‡§Æ‡§π‡§æ‡§® ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§π‡•à <end>   \n",
       "119006         <start> ‡§ú‡§ü‡§ø‡§≤‡§§‡§æ ‡§π‡•ã‡§§‡•Ä ‡§π‡•à ‡§¶‡•É‡§∂‡•ç‡§Ø ‡§≠‡§æ‡§∑‡§æ ‡§ï‡•Ä <end>   \n",
       "\n",
       "        english_sentence_length  hindi_sentence_length  \n",
       "87326                        12                     11  \n",
       "117649                        6                      5  \n",
       "37737                         8                      8  \n",
       "90347                         4                      5  \n",
       "119006                        4                      6  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['english_sentence'] = train['english_sentence'].apply(lambda x: '<start> '+str(' '.join(x.split()[:MAX_WORDS]))+' <end>')\n",
    "train['hindi_sentence'] = train['hindi_sentence'].apply(lambda x: '<start> '+str(' '.join(x.split()[:MAX_WORDS]))+' <end>')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS=MAX_WORDS+2 #add 2 for <start>,<end> token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and pad english sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizerE = Tokenizer(num_words=VOCAB, \n",
    "                       oov_token='<OOV>', \n",
    "                       lower=True, \n",
    "                       filters='#$!\"%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "\n",
    "tokenizerE.fit_on_texts(train['english_sentence'])\n",
    "\n",
    "eng_inp = tokenizerE.texts_to_sequences(train['english_sentence'])\n",
    "\n",
    "eng_inp = pad_sequences(eng_inp,\n",
    "                        maxlen=MAX_WORDS,\n",
    "                        truncating='post',\n",
    "                        padding='post',\n",
    "                        dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and pad hindi sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizerH = Tokenizer(num_words=VOCAB,\n",
    "                       oov_token='<OOV>',\n",
    "                       lower=True,\n",
    "                       filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "\n",
    "tokenizerH.fit_on_texts(train['hindi_sentence'])\n",
    "\n",
    "hin_inp=tokenizerH.texts_to_sequences(train['hindi_sentence'])\n",
    "\n",
    "hin_inp=pad_sequences(hin_inp,\n",
    "                      maxlen=MAX_WORDS,\n",
    "                      truncating='post',\n",
    "                      padding='post',\n",
    "                      dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating reverse hindi dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_hin_dict = dict(map(reversed, tokenizerH.word_index.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating datatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(eng_inp, hin_inp, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(BATCH_SIZE,drop_remainder=True)\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):    \n",
    "    def __init__(self,vocab,batch_size):\n",
    "        super(Encoder,self).__init__()  \n",
    "        self.vocab=vocab\n",
    "        self.batch_size=batch_size\n",
    "        self.embed=Embedding(self.vocab,256)\n",
    "        self.gru=Bidirectional(GRU(256,return_state=True,return_sequences=True,\n",
    "                                   recurrent_initializer='glorot_uniform',dropout=0.5))\n",
    "        self.gru1=Bidirectional(GRU(256,return_state=True,return_sequences=True,\n",
    "                                   recurrent_initializer='glorot_uniform',dropout=0.5))\n",
    "    \n",
    "    def call(self,encoder_inp,hidden):\n",
    "        encoder_inp=self.embed(encoder_inp)       \n",
    "        _,state_htmp,state_ctmp=self.gru(encoder_inp,initial_state=hidden)\n",
    "        encoder_out,state_h,state_c=self.gru1(encoder_inp,initial_state=[state_htmp,state_ctmp])\n",
    "        return encoder_out,tf.concat([state_h,state_c],axis=1)\n",
    "    \n",
    "    def initialise_hidden_unit(self):\n",
    "        return [tf.zeros((self.batch_size,256)) for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=Encoder(VOCAB,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):   \n",
    "    def __init__(self,vocab):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.vocab=vocab\n",
    "        self.embed=Embedding(self.vocab,256)\n",
    "        self.dense=Dense(512)\n",
    "        self.dense1=Dense(512)\n",
    "        self.dense2=Dense(1)\n",
    "        self.gru=GRU(512,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform',\n",
    "                     dropout=0.5)\n",
    "        self.dense3=Dense(self.vocab)\n",
    "        \n",
    "    def call(self,decoder_inp,encoder_out,carry):       \n",
    "        decoder_inp=self.embed(decoder_inp)\n",
    "        carry=tf.expand_dims(carry,1)\n",
    "#----------------------------------------------------------------\n",
    "        #attention\n",
    "        score=self.dense2(tf.math.tanh(self.dense1(encoder_out)+self.dense(carry)))\n",
    "        attention_weights=tf.nn.softmax(score,axis=1)\n",
    "        context_vector=tf.math.reduce_sum(attention_weights*encoder_out,axis=1,keepdims=True)\n",
    "        merged_vector=tf.concat([context_vector,decoder_inp],axis=-1)\n",
    "#-----------------------------------------------------------------        \n",
    "        decoder_out,decoder_state=self.gru(merged_vector)\n",
    "        \n",
    "        decoder_out=tf.reshape(decoder_out,(-1,decoder_out.shape[2]))\n",
    "        decoder_out=self.dense3(decoder_out)\n",
    "        return decoder_out,decoder_state,attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder=Decoder(VOCAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoint_dir = CHECKPOINT_PATH\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train(inp,out,hidden,MAX_WORDS):\n",
    "    loss=0\n",
    "    with tf.GradientTape() as tape:\n",
    "        eo,hidden=encoder(inp,hidden)\n",
    "        h=hidden       \n",
    "        hi=tf.expand_dims([tokenizerH.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "        hi=tf.cast(hi,'float32')        \n",
    "        for i in range (1,MAX_WORDS):           \n",
    "            do,ds,_=decoder(hi,eo,h) \n",
    "            loss+=loss_function(out[:, i], do)\n",
    "            hi=tf.expand_dims(out[:, i], 1) \n",
    "            hi=tf.cast(hi,'float32')    \n",
    "    variables = encoder.trainable_variables+decoder.trainable_variables\n",
    "    gradients=tape.gradient(loss, variables) \n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    batch_loss = loss / MAX_WORDS\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test(inp_,out_,hidden_,MAX_WORDS):\n",
    "    loss_=0\n",
    "    eo_,hidden_=encoder(inp_,hidden_)\n",
    "    h_=hidden_       \n",
    "    hi_=tf.expand_dims([tokenizerH.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "    hi_=tf.cast(hi_,'float32')        \n",
    "    for i in range (1,MAX_WORDS):           \n",
    "        do_,ds_,_=decoder(hi_,eo_,h_) \n",
    "        loss_+=loss_function(out_[:, i], do_)\n",
    "        hi_=tf.expand_dims(out_[:, i], 1) \n",
    "        hi_=tf.cast(hi_,'float32')    \n",
    "    batch_loss_ = loss_ / MAX_WORDS\n",
    "    return batch_loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the training and testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Train Loss = 4.0092\tVal Loss = 3.7056\tTime taken = 105.73 secs\n",
      "Starting epoch 2\n",
      "Train Loss = 3.5157\tVal Loss = 3.4562\tTime taken = 92.80 secs\n",
      "Starting epoch 3\n",
      "Train Loss = 3.2791\tVal Loss = 3.2927\tTime taken = 92.61 secs\n",
      "Starting epoch 4\n",
      "Train Loss = 3.0564\tVal Loss = 3.1344\tTime taken = 91.73 secs\n",
      "Starting epoch 5\n",
      "Train Loss = 2.8119\tVal Loss = 2.9908\tTime taken = 92.05 secs\n",
      "Starting epoch 6\n",
      "Train Loss = 2.5528\tVal Loss = 2.8794\tTime taken = 92.03 secs\n",
      "Starting epoch 7\n",
      "Train Loss = 2.3121\tVal Loss = 2.8070\tTime taken = 92.22 secs\n",
      "Starting epoch 8\n",
      "Train Loss = 2.0998\tVal Loss = 2.7641\tTime taken = 91.99 secs\n",
      "Starting epoch 9\n",
      "Train Loss = 1.9153\tVal Loss = 2.7490\tTime taken = 91.97 secs\n",
      "Starting epoch 10\n",
      "Train Loss = 1.7522\tVal Loss = 2.7428\tTime taken = 91.91 secs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for epoch in range(10):   \n",
    "    print(\"Starting epoch {}\".format(epoch+1))\n",
    "    start=time.time()  \n",
    "    \n",
    "    hidden = encoder.initialise_hidden_unit()\n",
    "    total_loss = 0  \n",
    "    \n",
    "    hidden_ = encoder.initialise_hidden_unit()\n",
    "    total_loss_ = 0 \n",
    "#---------------------------------------------------------------------------------------------- \n",
    "    for x_batch,y_batch in dataset_train: \n",
    "        batch_loss = train(x_batch, y_batch, hidden,MAX_WORDS) #calling the training loop\n",
    "        total_loss += batch_loss\n",
    "#----------------------------------------------------------------------------------------------\n",
    "    for x_batch,y_batch in dataset_test: \n",
    "        batch_loss_ = test(x_batch, y_batch, hidden_,MAX_WORDS) #calling the testing loop\n",
    "        total_loss_ += batch_loss_\n",
    "#----------------------------------------------------------------------------------------------\n",
    "        \n",
    "    end=time.time()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    print('Train Loss = {:.4f}\\tVal Loss = {:.4f}\\tTime taken = {:.2f} secs'.format(total_loss/(SAMPLES*0.9//BATCH_SIZE),\n",
    "                                                                                    total_loss_/(SAMPLES*0.1//BATCH_SIZE),\n",
    "                                                                                    (end-start))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>TESTING</f>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test,plot=False):\n",
    "    \n",
    "    test=' '.join(test.split()[:MAX_WORDS-2])\n",
    "    test=\"<start> \"+test+\" <end>\"\n",
    "    test_ori=test\n",
    "    \n",
    "    test=tokenizerE.texts_to_sequences([test])\n",
    "    test=pad_sequences(test,\n",
    "                       maxlen=MAX_WORDS,\n",
    "                       truncating='post',\n",
    "                       padding='post',\n",
    "                       dtype='float32')\n",
    "\n",
    "    hidd=[tf.zeros((1,256)) for i in range(2)]\n",
    "    testenou,testenhi=encoder(test,hidd)\n",
    "    testhi=tf.expand_dims([tokenizerH.word_index['<start>']] * 1, 1)\n",
    "    testhi=tf.cast(testhi,'float32')\n",
    "    pred=''\n",
    "\n",
    "    apn_att_wt=np.zeros((MAX_WORDS-1,MAX_WORDS))\n",
    "    for i in range (1,MAX_WORDS):           \n",
    "        testdo,testds,att_wt=decoder(testhi,testenou,testenhi)    \n",
    "        apn_att_wt[i-1]=tf.reshape(att_wt[0],(MAX_WORDS))\n",
    "        m=tf.math.argmax(testdo[0])\n",
    "\n",
    "        pred+=rev_hin_dict[m.numpy()]+' '\n",
    "        testhi=tf.expand_dims([m.numpy()] * 1, 1)\n",
    "        testhi=tf.cast(testhi,'float32')\n",
    "        testenhi=testds\n",
    "\n",
    "    pre=''\n",
    "    for word in pred.split():\n",
    "        if word=='<end>':\n",
    "            break\n",
    "        else:\n",
    "            pre+=word+' '\n",
    "\n",
    "    if plot==True:       \n",
    "        attention=np.reshape(apn_att_wt,(MAX_WORDS,MAX_WORDS-1))\n",
    "        fig = plt.figure(figsize=(5,5))\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        ax.matshow(attention, cmap='viridis')\n",
    "        plt.show()         \n",
    "\n",
    "    print(f\"INPUT  =  {test_ori}\\nHINDI  =  {pre}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT  =  <start> In jail, Bhagat singh and his friends were on hunger strike for 64 days. <end>\n",
      "HINDI  =  ‡§ú‡•á‡§≤ ‡§Æ‡•á‡§Ç ‡§≠‡§ó‡§§ ‡§∏‡§ø‡§Ç‡§π ‡§î‡§∞ ‡§¨‡§æ‡§ï‡§ø ‡§∏‡§æ‡§•‡§ø‡§Ø‡•ã ‡§®‡•á ‡•¨‡•™ ‡§¶‡§ø‡§®‡•ã ‡§§‡§ï ‡§≠‡•Ç‡§ñ ‡§π‡§¶‡•ç‡§§‡§æ‡§≤ ‡§ï‡§ø‡•§ \n"
     ]
    }
   ],
   "source": [
    "evaluate(\"In jail, Bhagat singh and his friends were on hunger strike for 64 days.\", plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
