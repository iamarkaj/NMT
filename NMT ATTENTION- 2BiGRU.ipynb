{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import unidecode\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense,GRU,Bidirectional,Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_word=18+2\n",
    "samples=50000\n",
    "vocab=20000\n",
    "BATCH=64\n",
    "\n",
    "import csv\n",
    "engtxt=[]\n",
    "hintxt=[]\n",
    "with open ('data.csv','r',encoding='utf8') as f:\n",
    "    l=csv.reader(f)\n",
    "    for row in l:\n",
    "        engtxt.append(row[1])\n",
    "        hintxt.append(row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eng=engtxt[:50000]\n",
    "hin=hintxt[:50000]\n",
    "#hin=[unidecode.unidecode(sent) for sent in hin]\n",
    "\n",
    "for i in range (4,5):\n",
    "    print(eng[i]+'--->'+hin[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eng=['#','$']+eng\n",
    "tokenizerE=Tokenizer(num_words=vocab,oov_token='<OOV>',lower=True,filters='!\"%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizerE.fit_on_texts(eng)\n",
    "eng=eng[2:]\n",
    "eng_seq=tokenizerE.texts_to_sequences(eng)\n",
    "eng_pad=pad_sequences(eng_seq,maxlen=max_word-2,truncating='post',padding='post')\n",
    "eng_inp=[list(np.concatenate(([tokenizerE.word_index['#']],l,[tokenizerE.word_index['$']]),axis=0)) for l in eng_pad]\n",
    "eng_inp=np.reshape(eng_inp,(samples,max_word)).astype('float32')\n",
    "\n",
    "hin=['#','$']+hin\n",
    "tokenizerB=Tokenizer(num_words=vocab,oov_token='<OOV>',lower=False,filters='!\"%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizerB.fit_on_texts(hin)\n",
    "hin=hin[2:]\n",
    "hin_seq=tokenizerB.texts_to_sequences(hin)\n",
    "hin_pad=pad_sequences(hin_seq,maxlen=max_word-2,truncating='post',padding='post')\n",
    "hin_inp=[list(np.concatenate(([tokenizerB.word_index['#']],l,[tokenizerB.word_index['$']]),axis=0)) for l in hin_pad]\n",
    "hin_inp=np.reshape(hin_inp,(samples,max_word)).astype('float32')\n",
    "\n",
    "rev_hin_dict = dict(map(reversed, tokenizerB.word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):    \n",
    "    def __init__(self,vocab,BATCH):\n",
    "        super(Encoder,self).__init__()  \n",
    "        self.vocab=vocab\n",
    "        self.BATCH=BATCH\n",
    "        self.embed=Embedding(self.vocab,256)\n",
    "        self.gru=Bidirectional(GRU(256,return_state=True,return_sequences=True,\n",
    "                                   recurrent_initializer='glorot_uniform',dropout=0.5))\n",
    "        self.gru1=Bidirectional(GRU(256,return_state=True,return_sequences=True,\n",
    "                                   recurrent_initializer='glorot_uniform',dropout=0.5))\n",
    "    \n",
    "    def call(self,encoder_inp,hidden):\n",
    "        encoder_inp=self.embed(encoder_inp)       \n",
    "        _,state_htmp,state_ctmp=self.gru(encoder_inp,initial_state=hidden)\n",
    "        encoder_out,state_h,state_c=self.gru1(encoder_inp,initial_state=[state_htmp,state_ctmp])\n",
    "        return encoder_out,tf.concat([state_h,state_c],axis=1)\n",
    "    \n",
    "    def initialise_hidden_unit(self):\n",
    "        return [tf.zeros((self.BATCH,256)) for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):   \n",
    "    def __init__(self,vocab):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.vocab=vocab\n",
    "        self.embed=Embedding(self.vocab,256)\n",
    "        self.dense=Dense(512)\n",
    "        self.dense1=Dense(512)\n",
    "        self.dense2=Dense(1)\n",
    "        self.gru=GRU(512,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform',\n",
    "                     dropout=0.5)\n",
    "        self.dense3=Dense(self.vocab)\n",
    "        \n",
    "    def call(self,decoder_inp,encoder_out,carry):       \n",
    "        decoder_inp=self.embed(decoder_inp)\n",
    "        carry=tf.expand_dims(carry,1)\n",
    "#----------------------------------------------------------------\n",
    "#attention\n",
    "        score=self.dense2(tf.math.tanh(self.dense1(encoder_out)+self.dense(carry)))\n",
    "        attention_weights=tf.nn.softmax(score,axis=1)\n",
    "        context_vector=tf.math.reduce_sum(attention_weights*encoder_out,axis=1,keepdims=True)\n",
    "        merged_vector=tf.concat([context_vector,decoder_inp],axis=-1)\n",
    "#-----------------------------------------------------------------        \n",
    "        decoder_out,decoder_state=self.gru(merged_vector)\n",
    "        \n",
    "        decoder_out=tf.reshape(decoder_out,(-1,decoder_out.shape[2]))\n",
    "        decoder_out=self.dense3(decoder_out)\n",
    "        return decoder_out,decoder_state,attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=Encoder(vocab,BATCH)\n",
    "decoder=Decoder(vocab)\n",
    "\n",
    "optimizer=tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create checkpoint\n",
    "import os\n",
    "checkpoint_dir = 'E:/Jupyter files/ROUGH/training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop\n",
    "@tf.function\n",
    "def train(inp,out,hidden,vocab,max_word):\n",
    "    loss=0\n",
    "    with tf.GradientTape() as tape:\n",
    "        eo,hidden=encoder(inp,hidden)\n",
    "        c=hidden       \n",
    "        bi=tf.expand_dims([tokenizerB.word_index['#']] * BATCH, 1)\n",
    "        bi=tf.cast(bi,'float32')        \n",
    "        for i in range (1,max_word):           \n",
    "            do,ds,_=decoder(bi,eo,c) \n",
    "            loss+=loss_function(out[:, i], do)\n",
    "            bi=tf.expand_dims(out[:, i], 1) \n",
    "            bi=tf.cast(bi,'float32')    \n",
    "    variables = encoder.trainable_variables+decoder.trainable_variables\n",
    "    gradients=tape.gradient(loss, variables) \n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    batch_loss = loss / max_word\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change epoch to 15\n",
    "EPOCH=15\n",
    "e=eng_inp\n",
    "b=hin_inp\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((e,b)).shuffle(samples)\n",
    "dataset = dataset.batch(BATCH,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for epoch in range(EPOCH):   \n",
    "    print(\"Starting epoch {}\".format(epoch+1))\n",
    "    time1=time.time()\n",
    "    \n",
    "    hidden = encoder.initialise_hidden_unit()\n",
    "    total_loss = 0    \n",
    "    for x_batch,y_batch in dataset: \n",
    "#---------------------------------------------------- \n",
    "#calling the training loop\n",
    "        batch_loss = train(x_batch, y_batch, hidden,vocab,max_word)\n",
    "#----------------------------------------------------\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    time2=time.time()\n",
    "    timediff=time2-time1\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('loss = {:.4f}\\ttime taken = {:.2f} secs'.format(total_loss/(samples//BATCH),timediff))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"you are a happy man\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ori=test\n",
    "#--------------------------------------------------------\n",
    "test=[test]\n",
    "test=tokenizerE.texts_to_sequences(test)\n",
    "test=np.array(test).astype('float32')\n",
    "test=pad_sequences(test,maxlen=max_word-2,truncating='post',padding='post')\n",
    "tst=[list(np.concatenate(([tokenizerE.word_index['#']],test[0],[tokenizerE.word_index['$']]),axis=0))]\n",
    "tst=np.reshape(tst,(max_word)).astype('float32')\n",
    "#--------------------------------------------------------\n",
    "test=tst\n",
    "test=np.reshape(test,(1,max_word))\n",
    "hidd=[tf.zeros((1,256)) for i in range(2)]\n",
    "testenou,testenhi=encoder(test,hidd)\n",
    "testbi=tf.expand_dims([tokenizerB.word_index['#']] * 1, 1)\n",
    "testbi=tf.cast(testbi,'float32')\n",
    "pred=''\n",
    "apn_att_wt=np.array([])\n",
    "for i in range (1,max_word):           \n",
    "    testdo,testds,att_wt=decoder(testbi,testenou,testenhi)    \n",
    "    apn_att_wt=np.append(apn_att_wt,att_wt)\n",
    "    m=tf.math.argmax(testdo[0])\n",
    " \n",
    "    pred+=rev_hin_dict[m.numpy()]+' '\n",
    "    testbi=tf.expand_dims([m.numpy()] * 1, 1)\n",
    "    testbi=tf.cast(testbi,'float32')\n",
    "    testenhi=testds\n",
    "#----------------------------------------------------------   \n",
    "pre=''\n",
    "for word in pred.split():\n",
    "    if word=='$':\n",
    "        break\n",
    "    else:\n",
    "        pre+=word+' '\n",
    "print(f\"eng  =  {test_ori}\\npred  =  {pre}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#to evaluate within training data\n",
    "p=55\n",
    "for t in range(p,p+1):\n",
    "    test=e[t]\n",
    "    test=np.reshape(test,(1,max_word))\n",
    "    hidd=[tf.zeros((1,256)) for i in range(2)]\n",
    "    testenou,testenhi=encoder(test,hidd)\n",
    "    testbi=tf.expand_dims([tokenizerB.word_index['#']] * 1, 1)\n",
    "    testbi=tf.cast(testbi,'float32')\n",
    "    pred=''\n",
    "    apn_att_wt=np.array([])\n",
    "    for i in range (1,max_word):           \n",
    "        testdo,testds,att_wt=decoder(testbi,testenou,testenhi)    \n",
    "        apn_att_wt=np.append(apn_att_wt,att_wt)\n",
    "        m=tf.math.argmax(testdo[0])\n",
    " \n",
    "        pred+=rev_hin_dict[m.numpy()]+' '\n",
    "        testbi=tf.expand_dims([m.numpy()] * 1, 1)\n",
    "        testbi=tf.cast(testbi,'float32')\n",
    "        testenhi=testds\n",
    "    print(f\"eng  =  {eng[t]}\\nhin  =  {hin[t]}\\npred  =  {pred}\")\n",
    "\n",
    "    sentence=[word for word in eng[t].split()]\n",
    "    predicted_sentence=[word for word in pred.split()]\n",
    "#-------------------------------------------------------\n",
    "#to create the plot\n",
    "attention=np.reshape(apn_att_wt,(max_word,max_word-1))\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.matshow(attention, cmap='viridis')\n",
    "fontdict = {'fontsize': 14}\n",
    "\n",
    "ax.set_xticklabels([''] + predicted_sentence, fontdict=fontdict, rotation=90)\n",
    "ax.set_yticklabels([''] + sentence, fontdict=fontdict)\n",
    "\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "plt.show()\n",
    "#--------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
